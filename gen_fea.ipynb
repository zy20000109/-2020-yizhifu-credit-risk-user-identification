{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前期处理"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from category_encoders.count import CountEncoder\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "读取原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#请换成您的数据所在路径\n",
    "DATA_PATH=\"F:/Desktop/data/\"\n",
    "train_label = pd.read_csv(DATA_PATH+'train_label.csv')\n",
    "train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
    "test_a_base = pd.read_csv(DATA_PATH+'test_a_base.csv')\n",
    "test_b_base=pd.read_csv(DATA_PATH+\"testb_base.csv\")\n",
    "train_op = pd.read_csv(DATA_PATH+'train_op.csv')\n",
    "train_trans = pd.read_csv(DATA_PATH+'train_trans.csv')\n",
    "test_a_op = pd.read_csv(DATA_PATH+'test_a_op.csv')\n",
    "test_b_op=pd.read_csv(DATA_PATH+\"testb_op.csv\")\n",
    "test_a_trans = pd.read_csv(DATA_PATH+'test_a_trans.csv')\n",
    "test_b_trans=pd.read_csv(DATA_PATH+\"testb_trans.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义生成时间戳函数\n",
    "def transform_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400*day+3600*hour+60*minute+second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#复制数据\n",
    "train_df = train_base.copy()\n",
    "test_a_df = test_a_base.copy()\n",
    "test_b_df = test_b_base.copy()\n",
    "train_df = train_label.merge(train_df, on=['user'], how='left')\n",
    "del train_base, test_a_base, test_b_base\n",
    "#合并数据\n",
    "op_df = pd.concat([train_op, test_a_op,test_b_op], axis=0, ignore_index=True)\n",
    "trans_df = pd.concat([train_trans, test_a_trans,test_b_trans], axis=0, ignore_index=True)\n",
    "data = pd.concat([train_df, test_a_df,test_b_df], axis=0, ignore_index=True)\n",
    "del train_op, test_a_op, test_b_op, train_df, test_a_df, test_b_df, train_trans, test_a_trans,test_b_trans\n",
    "# 时间维度的处理\n",
    "op_df['day'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "trans_df['day'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[0]))\n",
    "op_df['timestamp'] = op_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "trans_df['timestamp'] = trans_df['tm_diff'].apply(lambda x: transform_time(x))\n",
    "op_df['hour'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "trans_df['hour'] = trans_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[0]))\n",
    "trans_df['week'] = trans_df['day'].apply(lambda x: x % 7)\n",
    "op_df['min'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[1]))\n",
    "op_df['second'] = op_df['tm_diff'].apply(lambda x: int(x.split(' ')[2].split('.')[0].split(':')[2]))\n",
    "# 排序\n",
    "trans_df = trans_df.sort_values(by=['user', 'timestamp'])\n",
    "op_df = op_df.sort_values(by=['user', 'timestamp'])\n",
    "trans_df.reset_index(inplace=True, drop=True)\n",
    "op_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按小时对交易表进行时间分组\n",
    "trans_df[\"time\"]=\"time\"\n",
    "trans_df.loc[trans_df.hour<=6,\"time\"]=\"night\"\n",
    "trans_df.loc[(trans_df.hour>6)&(trans_df.hour<=12),\"time\"]=\"morning\"\n",
    "trans_df.loc[(trans_df.hour>12)&(trans_df.hour<=18),\"time\"]=\"afternoon\"\n",
    "trans_df.loc[trans_df.hour>18,\"time\"]=\"evening\"\n",
    "#按小时对op表进行时间分组\n",
    "op_df[\"time\"]=\"time\"\n",
    "op_df.loc[op_df.hour<=6,\"time\"]=\"night\"\n",
    "op_df.loc[(op_df.hour>6)&(op_df.hour<=12),\"time\"]=\"morning\"\n",
    "op_df.loc[(op_df.hour>12)&(op_df.hour<=18),\"time\"]=\"afternoon\"\n",
    "op_df.loc[op_df.hour>18,\"time\"]=\"evening\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除缺失过多的service3_level\n",
    "data.drop(['service3_level'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "base基础信息表特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#算术特征提取\n",
    "data['product7_fail_ratio'] = data['product7_fail_cnt'] / data['product7_cnt']\n",
    "data['city_count'] = data.groupby(['city'])['user'].transform('count')\n",
    "data['province_count'] = data.groupby(['province'])['user'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#对年龄和使用时长进行woe编码\n",
    "def woe_iv_of_value(value):\n",
    "    df=data[~data['label'].isnull()].copy()\n",
    "    df['label_0'] = df['label'].map(lambda x: 1 if x == 0 else 0)\n",
    "    df['label_1'] = df['label'].map(lambda x: 1 if x == 1 else 0)\n",
    "    woe_iv_df_1 = df.groupby(value)['label_0'].agg({'sum'}).reset_index()\n",
    "    woe_iv_df_1.columns=[value,\"cnt_0_of_{}\".format(value)]\n",
    "    woe_iv_df_2 = df.groupby(value)['label_1'].agg({'sum'}).reset_index()\n",
    "    woe_iv_df_2.columns=[value,\"cnt_1_of_{}\".format(value)]\n",
    "    woe_iv_df = pd.merge(woe_iv_df_1, woe_iv_df_2, how = 'left', on = value)\n",
    "    woe_iv_df['cnt_of_{}'.format(value)] = woe_iv_df[\"cnt_0_of_{}\".format(value)] + woe_iv_df[\"cnt_1_of_{}\".format(value)]\n",
    "    woe_iv_df['ratio_0_of_{}'.format(value)] = woe_iv_df[\"cnt_0_of_{}\".format(value)]/woe_iv_df[\"cnt_0_of_{}\".format(value)].sum()\n",
    "    woe_iv_df['ratio_1_of_{}'.format(value)] = woe_iv_df[\"cnt_1_of_{}\".format(value)]/woe_iv_df[\"cnt_1_of_{}\".format(value)].sum()\n",
    "    # 计算各个箱的woe值\n",
    "    woe_iv_df['woe_of_{}'.format(value)] = np.log(woe_iv_df['ratio_0_of_{}'.format(value)]/woe_iv_df['ratio_1_of_{}'.format(value)])\n",
    "    woe_iv_df['woe_of_{}'.format(value)].replace(-np.inf,-1,inplace=True)\n",
    "    woe_iv_df['woe_of_{}'.format(value)].replace(np.inf,1,inplace=True)\n",
    "#     woe_iv_df['iv_of_{}'.format(value)] = (woe_iv_df['ratio_0_of_{}'.format(value)] - woe_iv_df['ratio_1_of_{}'.format(value)]) * woe_iv_df['woe_of_{}'.format(value)]\n",
    "    woe_iv_df=woe_iv_df[[value,'woe_of_{}'.format(value)]]\n",
    "    return woe_iv_df\n",
    "\n",
    "woe_iv_list=[\"age\",\"using_time\"]\n",
    "for value in tqdm(woe_iv_list):\n",
    "    data=data.merge(woe_iv_of_value(value),on=value,how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 34.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 对base表中的类别特征进行LabelEncoder\n",
    "for col in tqdm([f for f in data.select_dtypes('object').columns if f not in ['user']]):\n",
    "    le = LabelEncoder()\n",
    "    data[col].fillna('-1', inplace=True)\n",
    "    data[col] = le.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5a9582ef344ee2980c9d9177c1b94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 对base表中的部分字段进行频数编码\n",
    "DATA_PATH=\"F:/Desktop/data/\"\n",
    "train_base = pd.read_csv(DATA_PATH+'train_base.csv')\n",
    "test_a_base = pd.read_csv(DATA_PATH+'test_a_base.csv')\n",
    "test_b_base=pd.read_csv(DATA_PATH+\"testb_base.csv\")\n",
    "df = pd.concat([train_base,test_a_base,test_b_base], axis=0, ignore_index=True)\n",
    "del train_base, test_a_base, test_b_base\n",
    "df_category=df.select_dtypes('object')\n",
    "\n",
    "df_category_nunique = df_category.nunique()\n",
    "A_cnt_features = [col for col in df_category_nunique.index if df_category_nunique.loc[col] > 5 and col!='user']\n",
    "frequency_fea = pd.DataFrame()\n",
    "frequency_fea['user'] = df_category['user'].values\n",
    "for col in tqdm_notebook(A_cnt_features):\n",
    "    df_category[col] = df_category[col].fillna(-999)\n",
    "    frequency_fea[col + '_cnt'] = df_category[col].map(df_category[col].value_counts())\n",
    "    \n",
    "data=data.merge(frequency_fea,on=\"user\",how=\"left\")\n",
    "del df,df_category,df_category_nunique"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trans交易信息表特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户金额特征函数\n",
    "def gen_user_amount_features(df):\n",
    "    group_df = df.groupby(['user'])['amount'].agg([\n",
    "        ('user_amount_mean', 'mean'),\n",
    "        ('user_amount_std','std'),\n",
    "        ('user_amount_max', 'max'),\n",
    "        ('user_amount_min', 'min'),\n",
    "        ('user_amount_sum', 'sum'),\n",
    "        ('user_amount_med', 'median'),\n",
    "        ('user_amount_cnt', 'count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#用户金额特征提取\n",
    "data = data.merge(gen_user_amount_features(trans_df), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "#定义用户字段唯一值函数\n",
    "def gen_user_nunique_features(df, value, prefix):\n",
    "    group_df = df.groupby(['user'])[value].agg([\n",
    "        ('user_{}_{}_nuniq'.format(prefix, value), 'nunique')]\n",
    "    ).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#交易表字段唯一值特征提取\n",
    "for col in tqdm(['day', 'platform', 'tunnel_in', 'tunnel_out', 'type1', 'type2', 'ip', 'ip_3']):\n",
    "    data = data.merge(gen_user_nunique_features(df=trans_df, value=col, prefix='trans'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用户日均交易金额\n",
    "data['user_amount_per_days'] = data['user_amount_sum'] / data['user_trans_day_nuniq']\n",
    "#用户平均每次交易金额\n",
    "data['user_amount_per_cnt'] = data['user_amount_sum'] / data['user_amount_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户分组金额交易函数\n",
    "def gen_user_group_amount_features(df, value):\n",
    "    group_df = df.pivot_table(index='user',\n",
    "                              columns=value,\n",
    "                              values='amount',\n",
    "                              dropna=False,\n",
    "                              aggfunc=['count', 'sum'])\n",
    "    group_df.columns = ['user_{}_{}_amount_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n",
    "    group_df.reset_index(inplace=True)\n",
    "\n",
    "    return group_df\n",
    "\n",
    "#生成用户关于平台的分组交易金额特征\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='platform'), on=['user'], how='left')\n",
    "#生成用户关于type1的分组交易金额特征\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='type1'), on=['user'], how='left')\n",
    "#生成用户关于type2的分组交易金额特征\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='type2'), on=['user'], how='left')\n",
    "#生成用户关于time的分组交易金额特征\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='time'), on=['user'], how='left')\n",
    "#生成用户关于week的分组交易金额特征\n",
    "data = data.merge(gen_user_group_amount_features(df=trans_df, value='week'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义交易天数金额窗口周期函数\n",
    "def gen_user_window_amount_features(df, window):\n",
    "    group_df = df[df['day']>window].groupby('user')['amount'].agg([\n",
    "        ('user_amount_mean_{}d'.format(window), 'mean'),\n",
    "        ('user_amount_std_{}d'.format(window),'std'),\n",
    "        ('user_amount_max_{}d'.format(window),'max'),\n",
    "        ('user_amount_min_{}d'.format(window), 'min'),\n",
    "        ('user_amount_sum_{}d'.format(window),'sum'),\n",
    "        ('user_amount_med_{}d'.format(window),'median'),\n",
    "        ('user_amount_cnt_{}d'.format(window),'count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#提取7日窗口金额特征\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=7), on=['user'], how='left')\n",
    "#提取15日窗口金额特征\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=15), on=['user'], how='left')\n",
    "#提取23日窗口金额特征\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=23), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义交易小时金额窗口周期函数\n",
    "def gen_user_window_amount_features(df, window):\n",
    "    group_df = df[df['hour']>window].groupby('user')['amount'].agg([\n",
    "        ('user_amount_mean_{}h'.format(window), 'mean'),\n",
    "        ('user_amount_std_{}h'.format(window),'std'),\n",
    "        ('user_amount_max_{}h'.format(window),'max'),\n",
    "        ('user_amount_min_{}h'.format(window),'min'),\n",
    "        ('user_amount_sum_{}h'.format(window), 'sum'),\n",
    "        ('user_amount_med_{}h'.format(window), 'median'),\n",
    "        ('user_amount_cnt_{}h'.format(window), 'count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#提取6时窗口金额特征\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=6), on=['user'], how='left')\n",
    "#提取12时窗口金额特征\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=12), on=['user'], how='left')\n",
    "#提取18时窗口金额特征\n",
    "data = data.merge(gen_user_window_amount_features(df=trans_df, window=18), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义空值特征提取函数\n",
    "def gen_user_null_features(df, value, prefix):\n",
    "    df['is_null'] = 0\n",
    "    df.loc[df[value].isnull(), 'is_null'] = 1\n",
    "\n",
    "    group_df = df.groupby(['user'])['is_null'].agg([('user_{}_{}_null_cnt'.format(prefix, value), 'sum'),\n",
    "                                                    ('user_{}_{}_null_ratio'.format(prefix, value),'mean')]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#提取交易表ip地址缺失特征\n",
    "data = data.merge(gen_user_null_features(df=trans_df, value='ip', prefix='trans'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取type1为45a1168437c708ff的用户交易最小天数特征\n",
    "group_df = trans_df[trans_df['type1']=='45a1168437c708ff'].groupby(['user'])['day'].agg([('user_type1_45a1168437c708ff_min_day', 'min')]).reset_index()\n",
    "data = data.merge(group_df, on=['user'], how='left')\n",
    "del group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户每天每小时交易金额函数\n",
    "def per_hour_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('per_hour_amt_sum','sum'),\n",
    "        ('per_hour_amt_mean','mean'),\n",
    "        ('per_hour_amt_max','max'),\n",
    "        ('per_hour_amt_min','min'),\n",
    "        ('per_hour_amt_cnt','count'),\n",
    "        ('per_hour_amt_std','std')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户每天每小时交易金额特征\n",
    "data = data.merge(per_hour_amt(trans_df,value1=\"day\",value2=\"hour\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户来源与去向交易金额函数\n",
    "def tunnel_in_out_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('tunnel_in_out_amt_sum','sum'),\n",
    "        ('tunnel_in_out_amt_cnt','count'),\n",
    "        ('tunnel_in_out_amt_mean','mean'),\n",
    "        ('tunnel_in_out_amt_max','max'),\n",
    "        ('tunnel_in_out_amt_min','min'),\n",
    "        ('tunnel_in_out_amt_std','std'),\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户来源去向交易金额特征\n",
    "data = data.merge(tunnel_in_out_amt(trans_df,value1=\"tunnel_in\",value2=\"tunnel_out\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户日期ip金额函数\n",
    "def day_ip_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('day_ip_amt_sum','sum'),\n",
    "        ('day_ip_amt_mean','mean'),\n",
    "        ('day_ip_amt_std','std'),\n",
    "        ('day_ip_amt_max','max'),\n",
    "        ('day_ip_amt_min','min'),\n",
    "        ('day_ip_amt_cnt','count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户日期ip交易金额特征\n",
    "data = data.merge(day_ip_amt(trans_df,value1=\"day\",value2=\"ip\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户前后时间交易金额gap函数\n",
    "def amt_gap(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df['last_amount']=group_df.groupby('user')['amount'].shift(1)\n",
    "    group_df['amount_gap']=abs(group_df[\"amount\"]-group_df[\"last_amount\"])\n",
    "    group_df=group_df[['user','amount_gap']]\n",
    "    group_df=group_df.groupby('user')['amount_gap'].agg([\n",
    "        ('amt_gap_sum','sum'),\n",
    "        ('amt_gap_mean','mean'),\n",
    "        ('amt_gap_std','std'),\n",
    "        ('amt_gap_max','max'),\n",
    "        ('amt_gap_min','min')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户日期gap交易金额特征\n",
    "data = data.merge(amt_gap(trans_df,value1=\"day\",value2=\"hour\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义交易金额gap比率和函数\n",
    "def gap_amt_rate(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2,'amount']]\n",
    "    group_df=group_df.groupby(['user', 'day', 'hour'])['amount'].agg('sum').reset_index()\n",
    "    group_df['last_amount']=group_df.groupby('user')['amount'].shift(1)\n",
    "    group_df['gap_rate']=group_df['amount']/group_df['last_amount']\n",
    "    group_df=group_df[['user','gap_rate']]\n",
    "    group_df=group_df.groupby('user')['gap_rate'].agg('sum').reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户日期交易gap比率和特征\n",
    "data = data.merge(gap_amt_rate(trans_df,value1=\"day\",value2=\"hour\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户每天交易金额函数\n",
    "def per_day_amt(df,value):\n",
    "    group_df=df[['user',value, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('per_day_amt_mean','mean'),\n",
    "        ('per_day_amt_max','max'),\n",
    "        ('per_day_amt_min','min'),\n",
    "        ('per_day_amt_std','std')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户每天交易金额特征\n",
    "data = data.merge(per_day_amt(trans_df,value=\"day\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户前后时间交易金额gap函数\n",
    "def day_amt_gap(df,value):\n",
    "    group_df=df[['user',value, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df['last_amount']=group_df.groupby('user')['amount'].shift(1)\n",
    "    group_df['amount_gap']=abs(group_df[\"amount\"]-group_df[\"last_amount\"])\n",
    "    group_df=group_df[['user','amount_gap']]\n",
    "    group_df=group_df.groupby('user')['amount_gap'].agg([\n",
    "        ('day_amt_gap_mean','mean'),\n",
    "        ('day_amt_gap_std','std'),\n",
    "        ('day_amt_gap_max','max'),\n",
    "        ('day_amt_gap_min','min')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户每天交易金额gap特征\n",
    "data = data.merge(day_amt_gap(trans_df,value=\"day\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义每天交易金额gap比率和函数\n",
    "def day_gap_amt_rate(df,value):\n",
    "    group_df=df[['user',value,'amount']]\n",
    "    group_df=group_df.groupby(['user', value])['amount'].agg('sum').reset_index()\n",
    "    group_df['last_amount']=group_df.groupby('user')['amount'].shift(1)\n",
    "    group_df['day_gap_rate']=group_df['amount']/group_df['last_amount']\n",
    "    group_df=group_df[['user','day_gap_rate']]\n",
    "    group_df=group_df.groupby('user')['day_gap_rate'].agg('sum').reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户每天交易金额gap比率和特征\n",
    "data = data.merge(day_gap_amt_rate(trans_df,value=\"day\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户日期type1金额函数\n",
    "def day_type_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('day_type1_amt_sum','sum'),\n",
    "        ('day_type1_amt_mean','mean'),\n",
    "        ('day_type1_amt_std','std'),\n",
    "        ('day_type1_amt_max','max'),\n",
    "        ('day_type1_amt_min','min'),\n",
    "        ('day_type1_amt_cnt','count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户每天type1交易金额特征\n",
    "data = data.merge(day_type_amt(trans_df,value1=\"day\",value2=\"type1\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户日期type2金额函数\n",
    "def day_type2_amt(df,value1,value2):\n",
    "    group_df=df[['user',value1,value2, 'amount']]\n",
    "    group_df=group_df.groupby(['user',value1,value2])[\"amount\"].agg('sum').reset_index()\n",
    "    group_df=group_df[['user','amount']]\n",
    "    group_df=group_df.groupby('user')['amount'].agg([\n",
    "        ('day_type2_amt_sum','sum'),\n",
    "        ('day_type2_amt_mean','mean'),\n",
    "        ('day_type2_amt_std','std'),\n",
    "        ('day_type2_amt_max','max'),\n",
    "        ('day_type2_amt_min','min'),\n",
    "        ('day_type2_amt_cnt','count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#生成用户每天type交易金额特征\n",
    "data = data.merge(day_type2_amt(trans_df,value1=\"day\",value2=\"type2\"), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trans word2vec ...\n",
      "Start gen feat of platform ...\n",
      "Start gen feat of tunnel_in ...\n",
      "Start gen feat of tunnel_out ...\n",
      "Start gen feat of amount ...\n",
      "Start gen feat of type1 ...\n",
      "Start gen feat of type2 ...\n",
      "Start gen feat of ip ...\n",
      "Start gen feat of day ...\n",
      "Start gen feat of hour ...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "def w2v_feat(data_frame, feat, mode):\n",
    "    for i in feat:\n",
    "        if data_frame[i].dtype != 'object':\n",
    "            data_frame[i] = data_frame[i].astype(str)\n",
    "    data_frame.fillna('nan', inplace=True)\n",
    "\n",
    "    print(f'Start {mode} word2vec ...')\n",
    "    model = Word2Vec(data_frame[feat].values.tolist(), size=5, window=2, min_count=1,\n",
    "                     workers=multiprocessing.cpu_count(), iter=10)\n",
    "    stat_list = ['min', 'max', 'mean', 'std']\n",
    "    new_all = pd.DataFrame()\n",
    "    for m, t in enumerate(feat):\n",
    "        print(f'Start gen feat of {t} ...')\n",
    "        tmp = []\n",
    "        for i in data_frame[t].unique():\n",
    "            tmp_v = [i]\n",
    "            tmp_v.extend(model[i])\n",
    "            tmp.append(tmp_v)\n",
    "        tmp_df = pd.DataFrame(tmp)\n",
    "        w2c_list = [f'w2c_trans_{t}_{n}' for n in range(5)]\n",
    "        tmp_df.columns = [t] + w2c_list\n",
    "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
    "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
    "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
    "        if m == 0:\n",
    "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
    "        else:\n",
    "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
    "    return new_all\n",
    "\n",
    "#生成word2vec特征\n",
    "trans_feat=[\"platform\",\"tunnel_in\",\"tunnel_out\",\"amount\",\"type1\",\"type2\",\"ip\",\"day\",\"hour\"]\n",
    "data=data.merge(w2v_feat(trans_df,trans_feat,'trans'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对交易表部分字段进行CountEncoder\n",
    "from category_encoders.count import CountEncoder\n",
    "for i in [\"ip\",\"ip_3\",\"amount\"]:\n",
    "    trans_df[\"count_{}_trans\".format(i)]=CountEncoder().fit_transform(trans_df[i])\n",
    "    group_df=trans_df.groupby('user')[\"count_{}_trans\".format(i)].agg([\n",
    "        (\"count_{}_trans_max\".format(i),'max'),\n",
    "        (\"count_{}_trans_min\".format(i), 'min'),\n",
    "        (\"count_{}_trans_mean\".format(i),'mean'),\n",
    "        (\"count_{}_trans_std\".format(i),'std')\n",
    "    ]).reset_index()\n",
    "    data=data.merge(group_df,on=\"user\",how=\"left\")\n",
    "    \n",
    "trans_df.drop(columns=trans_df.columns[-3:],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f30448fc1e44c2a55d2b9040d3a49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58aebd1fcc94406bb2644b1ff7649459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#交易信息表部分字段频数编码\n",
    "trans_df_category=trans_df.drop(columns=[\"tm_diff\",\"time\"])\n",
    "trans_df_category_nunique = trans_df_category.nunique()\n",
    "A_cnt_features = [col for col in trans_df_category_nunique.index if trans_df_category_nunique.loc[col] > 5 and col!='user']\n",
    "len(A_cnt_features)\n",
    "frequency_fea = pd.DataFrame()\n",
    "frequency_fea['user'] = trans_df_category['user'].values\n",
    "for col in tqdm_notebook(A_cnt_features):\n",
    "    trans_df_category[col] = trans_df_category[col].fillna(-999)\n",
    "    frequency_fea[col + '_cnt'] = trans_df_category[col].map(trans_df_category[col].value_counts())\n",
    "    \n",
    "for i in tqdm_notebook(frequency_fea.columns[1:]):\n",
    "    group_df=frequency_fea[[\"user\",i]]\n",
    "    group_df=group_df.groupby(\"user\")[i].agg([\n",
    "        ('freq_{}_max'.format(i),'max'),\n",
    "        ('freq_{}_min'.format(i),'min'),\n",
    "        ('freq_{}_mean'.format(i),'mean'),\n",
    "        ('freq_{}_std'.format(i),'std')]).reset_index()\n",
    "    data=data.merge(group_df,on=\"user\",how=\"left\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "提取op操作信息表特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_tfidf_features(df, value):\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = TfidfVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_tfidf_{}_{}'.format(value, i) for i in range(10)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_countvec_features(df, value):\n",
    "    df[value] = df[value].astype(str)\n",
    "    df[value].fillna('-1', inplace=True)\n",
    "    group_df = df.groupby(['user']).apply(lambda x: x[value].tolist()).reset_index()\n",
    "    group_df.columns = ['user', 'list']\n",
    "    group_df['list'] = group_df['list'].apply(lambda x: ','.join(x))\n",
    "    enc_vec = CountVectorizer()\n",
    "    tfidf_vec = enc_vec.fit_transform(group_df['list'])\n",
    "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2020)\n",
    "    vec_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    vec_svd = pd.DataFrame(vec_svd)\n",
    "    vec_svd.columns = ['svd_countvec_{}_{}'.format(value, i) for i in range(10)]\n",
    "    group_df = pd.concat([group_df, vec_svd], axis=1)\n",
    "    del group_df['list']\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(gen_user_tfidf_features(df=op_df, value='op_mode'), on=['user'], how='left')\n",
    "data = data.merge(gen_user_tfidf_features(df=op_df, value='op_type'), on=['user'], how='left')\n",
    "data = data.merge(gen_user_countvec_features(df=op_df, value='op_mode'), on=['user'], how='left')\n",
    "data = data.merge(gen_user_countvec_features(df=op_df, value='op_type'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start op word2vec ...\n",
      "Start gen feat of op_type ...\n",
      "Start gen feat of op_mode ...\n",
      "Start gen feat of op_device ...\n",
      "Start gen feat of ip ...\n",
      "Start gen feat of channel ...\n",
      "Start gen feat of day ...\n",
      "Start gen feat of hour ...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "def w2v_feat(data_frame, feat, mode):\n",
    "    for i in feat:\n",
    "        if data_frame[i].dtype != 'object':\n",
    "            data_frame[i] = data_frame[i].astype(str)\n",
    "    data_frame.fillna('nan', inplace=True)\n",
    "\n",
    "    print(f'Start {mode} word2vec ...')\n",
    "    model = Word2Vec(data_frame[feat].values.tolist(), size=5, window=2, min_count=1,\n",
    "                     workers=multiprocessing.cpu_count(), iter=10)\n",
    "    stat_list = ['min', 'max', 'mean', 'std']\n",
    "    new_all = pd.DataFrame()\n",
    "    for m, t in enumerate(feat):\n",
    "        print(f'Start gen feat of {t} ...')\n",
    "        tmp = []\n",
    "        for i in data_frame[t].unique():\n",
    "            tmp_v = [i]\n",
    "            tmp_v.extend(model[i])\n",
    "            tmp.append(tmp_v)\n",
    "        tmp_df = pd.DataFrame(tmp)\n",
    "        w2c_list = [f'w2c_op_{t}_{n}' for n in range(5)]\n",
    "        tmp_df.columns = [t] + w2c_list\n",
    "        tmp_df = data_frame[['user', t]].merge(tmp_df, on=t)\n",
    "        tmp_df = tmp_df.drop_duplicates().groupby('user').agg(stat_list).reset_index()\n",
    "        tmp_df.columns = ['user'] + [f'{p}_{q}' for p in w2c_list for q in stat_list]\n",
    "        if m == 0:\n",
    "            new_all = pd.concat([new_all, tmp_df], axis=1)\n",
    "        else:\n",
    "            new_all = pd.merge(new_all, tmp_df, how='left', on='user')\n",
    "    return new_all\n",
    "\n",
    "#生成word2vec特征\n",
    "op_feat=[\"op_type\",\"op_mode\",\"op_device\",\"ip\",\"channel\",\"day\",\"hour\"]\n",
    "data=data.merge(w2v_feat(op_df,op_feat,'op'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对操作表部分字段进行CountEncoder\n",
    "from category_encoders.count import CountEncoder\n",
    "for i in [\"ip\",\"ip_3\",\"op_device\",\"op_type\",\"op_mode\"]:\n",
    "    op_df[\"count_{}_op\".format(i)]=CountEncoder().fit_transform(op_df[i])\n",
    "    group_df=op_df.groupby('user')[\"count_{}_op\".format(i)].agg([\n",
    "        (\"count_{}_op_max\".format(i),'max'),\n",
    "        (\"count_{}_op_min\".format(i), 'min'),\n",
    "        (\"count_{}_op_mean\".format(i),'mean'),\n",
    "        (\"count_{}_op_std\".format(i),'std')\n",
    "    ]).reset_index()\n",
    "    data=data.merge(group_df,on=\"user\",how=\"left\")\n",
    "    \n",
    "op_df.drop(columns=op_df.columns[-5:],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5300fb900bc4481c89495fe614a9cf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3318bf1f68a6490aaa2c967fba74cdcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#操作表频数编码\n",
    "op_df_category=op_df.drop(columns=[\"tm_diff\",\"time\"])\n",
    "op_df_category_nunique = op_df_category.nunique()\n",
    "A_cnt_features = [col for col in op_df_category_nunique.index if op_df_category_nunique.loc[col] > 5 and col!='user']\n",
    "len(A_cnt_features)\n",
    "frequency_fea = pd.DataFrame()\n",
    "frequency_fea['user'] = op_df_category['user'].values\n",
    "for col in tqdm_notebook(A_cnt_features):\n",
    "    op_df_category[col] = op_df_category[col].fillna(-999)\n",
    "    frequency_fea[col + '_cnt'] = op_df_category[col].map(op_df_category[col].value_counts())\n",
    "\n",
    "for i in tqdm_notebook(frequency_fea.columns[1:]):\n",
    "    group_df=frequency_fea[[\"user\",i]]\n",
    "    group_df=group_df.groupby(\"user\")[i].agg([\n",
    "       ('freq_{}_max'.format(i),'max'),\n",
    "        ('freq_{}_min'.format(i),'min'),\n",
    "        ('freq_{}_mean'.format(i),'mean'),\n",
    "        ('freq_{}_std'.format(i),'std')]).reset_index()\n",
    "    data=data.merge(group_df,on=\"user\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "#定义用户字段唯一值函数\n",
    "def gen_user_nunique_features(df, value, prefix):\n",
    "    group_df = df.groupby(['user'])[value].agg([\n",
    "        ('user_{}_{}_nuniq'.format(prefix, value),'nunique')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "#操作表字段唯一值特征提取\n",
    "for col in tqdm(['op_type', 'op_mode', 'ip', 'channel', 'ip_3', 'day']):\n",
    "    data = data.merge(gen_user_nunique_features(df=op_df, value=col, prefix='op'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义天数操作窗口周期函数\n",
    "def gen_user_window_op_features(df, window):\n",
    "    group_df = df[df['day']>window].groupby('user')['op_type'].agg([\n",
    "        ('user_op_cnt_{}d'.format(window),'count')\n",
    "    ]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "op_df[\"day\"]=op_df[\"day\"].astype(int)\n",
    "#提取5日窗口操作数特征\n",
    "data = data.merge(gen_user_window_op_features(df=op_df, window=5), on=['user'], how='left')\n",
    "#提取10日窗口操作数特征\n",
    "data = data.merge(gen_user_window_op_features(df=op_df, window=10), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义小时op窗口周期函数\n",
    "def gen_op_window_hour_features(df, window):\n",
    "    group_df = df[df['hour']>window].groupby('user')['op_type'].agg([\n",
    "        ('user_op_cnt_{}h'.format(window),'count')]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "op_df[\"hour\"]=op_df[\"hour\"].astype(int)\n",
    "#提取6时窗口op特征\n",
    "data = data.merge(gen_op_window_hour_features(df=op_df, window=6), on=['user'], how='left')\n",
    "#提取12时窗口op特征\n",
    "data = data.merge(gen_op_window_hour_features(df=op_df, window=12), on=['user'], how='left')\n",
    "#提取18时窗口op特征\n",
    "data = data.merge(gen_op_window_hour_features(df=op_df, window=18), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义用户分组op函数\n",
    "def gen_user_group_op_features(df, value):\n",
    "    group_df = df.pivot_table(index='user',\n",
    "                              columns=value,\n",
    "                              values='op_type',\n",
    "                              dropna=False,\n",
    "                              aggfunc=['count'])\n",
    "    group_df.columns = ['user_{}_{}_op_{}'.format(value, f[1], f[0]) for f in group_df.columns]\n",
    "    group_df.reset_index(inplace=True)\n",
    "\n",
    "    return group_df\n",
    "\n",
    "#生成用户关于time的分组op特征\n",
    "data = data.merge(gen_user_group_op_features(df=op_df, value='time'), on=['user'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每一个用户操作次数\n",
    "op_count = op_df[['user']]\n",
    "op_count['op_count'] = 1\n",
    "op_count = op_count.groupby('user').agg('count').reset_index()\n",
    "data = pd.merge(data, op_count, on='user', how='left')\n",
    "del op_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"op_cnt_per_day\"]=data[\"op_count\"]/data[\"user_op_day_nuniq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一个用户每一天每一小时操作数的最大值，最小值，均值，标准差\n",
    "def day_per_hour_cnt(df,value1,value2):\n",
    "    group_df = op_df[['user', 'day', 'hour']]\n",
    "    group_df['everyday_everyhour'] = 1\n",
    "    group_df = group_df.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "    group_df = group_df.drop(['day', 'hour'],axis = 1)\n",
    "    group_df = group_df.groupby('user')['everyday_everyhour'].agg([\n",
    "        ('day_per_hour_mean','mean'),\n",
    "        ('day_per_hour_max','max'),\n",
    "        ('day_per_hour_min','min'), \n",
    "        ('day_per_hour_std','std')]).reset_index()\n",
    "    return group_df\n",
    "\n",
    "data = data.merge(day_per_hour_cnt(op_df,\"day\",\"hour\"), on='user', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一个用户每一天操作数的最大值，最小值，均值，标准差\n",
    "frequence_one_day = op_df[['user', 'day']]\n",
    "frequence_one_day['everyday'] = 1\n",
    "frequence_one_day = frequence_one_day.groupby(['user', 'day']).agg('count').reset_index()\n",
    "frequence_one_day = frequence_one_day.drop('day', axis=1)\n",
    "frequence_one_day = frequence_one_day.groupby('user')['everyday'].agg([\n",
    "    ('per_day_mean','mean'),\n",
    "    ('per_day_max','max'),\n",
    "    ('per_day_min','min'),\n",
    "    ('per_day_std','std')]).reset_index()\n",
    "data = data.merge(frequence_one_day, on='user', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#morning op count\n",
    "frequence_morning = op_df[op_df.time==\"morning\"][['user', 'day','hour']]\n",
    "frequence_morning['everyday_morning'] = 1\n",
    "frequence_morning = frequence_morning.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_morning = frequence_morning.groupby(['user', 'day'])['everyday_morning'].agg('sum').reset_index()\n",
    "frequence_morning = frequence_morning[['user', 'everyday_morning']]\n",
    "frequence_morning = frequence_morning.groupby('user')['everyday_morning'].agg([\n",
    "    ('per_mor_mean','mean'),\n",
    "    ('per_mor_max','max'),\n",
    "    ('per_mor_min','min'),\n",
    "    ('per_mor_std','std')]).reset_index()\n",
    "data = data.merge(frequence_morning, on='user', how='left')\n",
    "del frequence_morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#afternoon op count\n",
    "frequence_afternoon = op_df[op_df.time==\"afternoon\"][['user', 'day','hour']]\n",
    "frequence_afternoon['everyday_afternoon'] = 1\n",
    "frequence_afternoon = frequence_afternoon.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_afternoon = frequence_afternoon.groupby(['user', 'day'])['everyday_afternoon'].agg('sum').reset_index()\n",
    "frequence_afternoon = frequence_afternoon[['user', 'everyday_afternoon']]\n",
    "frequence_afternoon = frequence_afternoon.groupby('user')['everyday_afternoon'].agg([\n",
    "    ('per_after_mean','mean'),\n",
    "    ('per_after_max','max'),\n",
    "    ('per_after_min','min'),\n",
    "    ('per_after_std','std')]).reset_index()\n",
    "data = data.merge(frequence_afternoon, on='user', how='left')\n",
    "del frequence_afternoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evening op count\n",
    "frequence_evening = op_df[op_df.time==\"evening\"][['user', 'day','hour']]\n",
    "frequence_evening['everyday_evening'] = 1\n",
    "frequence_evening = frequence_evening.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_evening = frequence_evening.groupby(['user', 'day'])['everyday_evening'].agg('sum').reset_index()\n",
    "frequence_evening = frequence_evening[['user', 'everyday_evening']]\n",
    "frequence_evening = frequence_evening.groupby('user')['everyday_evening'].agg([\n",
    "    ('per_eve_mean','mean'),\n",
    "    ('per_eve_max','max'),\n",
    "    ('per_eve_min','min'),\n",
    "    ('per_eve_std','std')]).reset_index()\n",
    "data = data.merge(frequence_evening, on='user', how='left')\n",
    "del frequence_evening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#night op count\n",
    "frequence_night = op_df[op_df.time==\"night\"][['user', 'day','hour']]\n",
    "frequence_night['everyday_night'] = 1\n",
    "frequence_night = frequence_night.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_night = frequence_night.groupby(['user', 'day'])['everyday_night'].agg('sum').reset_index()\n",
    "frequence_night = frequence_night[['user', 'everyday_night']]\n",
    "frequence_night = frequence_night.groupby('user')['everyday_night'].agg([\n",
    "    ('per_night_mean','mean'),\n",
    "    ('per_night_max','max'),\n",
    "    ('per_night_min','min'),\n",
    "    ('per_night_std','std')]).reset_index()\n",
    "data = data.merge(frequence_night, on='user', how='left')\n",
    "del frequence_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一个用户这一天距离前面一段时间操作数的最大值，最小值，均值，标准差\n",
    "frequence_one_day_gap = op_df[['user', 'day']]\n",
    "frequence_one_day_gap['everyday'] = 1\n",
    "frequence_one_day_gap = frequence_one_day_gap.groupby(['user', 'day']).agg('count').reset_index()\n",
    "frequence_one_day_gap['everyday_before'] = frequence_one_day_gap.groupby('user')['everyday'].shift(1)\n",
    "frequence_one_day_gap['everyday_before_gap'] = abs(frequence_one_day_gap['everyday'] - frequence_one_day_gap['everyday_before'])\n",
    "frequence_one_day_gap = frequence_one_day_gap[['user', 'everyday_before_gap']].groupby('user')['everyday_before_gap'].agg([\n",
    "    ('op_day_gap_mean','mean'),\n",
    "    ('op_day_gap_max','max'),\n",
    "    ('op_day_gap_min','min'),\n",
    "    ('op_day_gap_std','std')]).reset_index()\n",
    "data = data.merge(frequence_one_day_gap, on='user', how='left')\n",
    "del frequence_one_day_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一个用户这一小时距离前面一段时间操作数的最大值，最小值，均值，标准差\n",
    "frequence_one_hour_gap = op_df[['user', 'day', 'hour']]\n",
    "frequence_one_hour_gap['everyhour'] = 1\n",
    "frequence_one_hour_gap = frequence_one_hour_gap.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_one_hour_gap['everyhour_before'] = frequence_one_hour_gap.groupby('user')['everyhour'].shift(1)\n",
    "frequence_one_hour_gap['everyhour_before_gap'] = abs(frequence_one_hour_gap['everyhour'] - frequence_one_hour_gap['everyhour_before'])\n",
    "frequence_one_hour_gap = frequence_one_hour_gap[['user', 'everyhour_before_gap']].groupby('user')['everyhour_before_gap'].agg([\n",
    "    ('hour_gap_mean','mean'),\n",
    "    ('hour_gap_max','max'),\n",
    "    ('hour_gap_min','min'),\n",
    "    ('hour_gap_std','std')]).reset_index()\n",
    "data = data.merge(frequence_one_hour_gap, on='user', how='left')\n",
    "del frequence_one_hour_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一个用户这一天距离前面一段时间操作数的比率和\n",
    "frequence_one_day_rate = op_df[['user', 'day']]\n",
    "frequence_one_day_rate['everyday'] = 1\n",
    "frequence_one_day_rate = frequence_one_day_rate.groupby(['user', 'day']).agg('count').reset_index()\n",
    "frequence_one_day_rate['everyday_before'] = frequence_one_day_rate.groupby('user')['everyday'].shift(1)\n",
    "frequence_one_day_rate['everyday_before_rate'] = frequence_one_day_rate['everyday'] / frequence_one_day_rate['everyday_before']\n",
    "frequence_one_day_rate = frequence_one_day_rate[['user', 'everyday_before_rate']].groupby('user')['everyday_before_rate'].agg('sum').reset_index()\n",
    "data = data.merge(frequence_one_day_rate, on='user', how='left')\n",
    "del frequence_one_day_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一个用户这一小时距离前面一段时间操作数的比率和\n",
    "frequence_one_hour_rate = op_df[['user', 'day', 'hour']]\n",
    "frequence_one_hour_rate['everyhour'] = 1\n",
    "frequence_one_hour_rate = frequence_one_hour_rate.groupby(['user', 'day', 'hour']).agg('count').reset_index()\n",
    "frequence_one_hour_rate['everyhour_before'] = frequence_one_hour_rate.groupby('user')['everyhour'].shift(1)\n",
    "frequence_one_hour_rate['everyhour_before_rate'] = frequence_one_hour_rate['everyhour'] - frequence_one_hour_rate['everyhour_before']\n",
    "frequence_one_hour_rate = frequence_one_hour_rate[['user', 'everyhour_before_rate']].groupby('user')['everyhour_before_rate'].agg('sum').reset_index()\n",
    "data = data.merge(frequence_one_hour_rate, on='user', how='left')\n",
    "del frequence_one_hour_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每秒用户的操作最大值、最小值、均值、方差\n",
    "every_second = op_df[['user', 'second']]\n",
    "every_second['operation_second_caozuo'] = 1\n",
    "every_second = every_second.groupby(['user', 'second']).agg('count').reset_index()\n",
    "every_second = every_second[['user', 'operation_second_caozuo']]\n",
    "every_second = every_second.groupby('user')['operation_second_caozuo'].agg([\n",
    "    ('per_sec_max','max'),\n",
    "    ('per_sec_min','min'),\n",
    "    ('per_sec_mean','mean'),\n",
    "    ('per_sec_std','std')]).reset_index()\n",
    "data = data.merge(every_second, on='user', how='left')\n",
    "del every_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每分钟用户操作的最大值、最小值、均值、方差\n",
    "every_minute = op_df[['user', 'min']]\n",
    "every_minute['operation_minute_caozuo'] = 1\n",
    "every_minute = every_minute.groupby(['user', 'min']).agg('count').reset_index()\n",
    "every_minute = every_minute[['user', 'operation_minute_caozuo']]\n",
    "every_minute = every_minute.groupby('user')['operation_minute_caozuo'].agg([\n",
    "    ('per_minute_max','max'),\n",
    "    ('per_minute_min','min'),\n",
    "    ('per_minute_mean','mean'),\n",
    "    ('per_minute_std','std')]).reset_index()\n",
    "data = data.merge(every_minute, on='user', how='left')\n",
    "del every_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per day device nunique\n",
    "dev_per_day_cnt=op_df[op_df.op_device!=\"nan\"][[\"user\",\"day\",\"op_device\"]].drop_duplicates()\n",
    "dev_per_day_cnt=pd.DataFrame(dev_per_day_cnt.groupby([\"user\",\"day\"])[\"op_device\"].nunique()).reset_index()\n",
    "dev_per_day_cnt=dev_per_day_cnt[[\"user\",\"op_device\"]]\n",
    "dev_per_day_cnt=dev_per_day_cnt.groupby(['user'])[\"op_device\"].agg([\n",
    "    (\"dev_nun_mean\",\"mean\"),\n",
    "    (\"dev_nun_max\",\"max\"),\n",
    "    (\"dev_nun_min\",\"min\"),\n",
    "    (\"dev_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(dev_per_day_cnt, on='user', how='left')\n",
    "del dev_per_day_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per day hour device nunique\n",
    "dev_per_hour_cnt=op_df[op_df.op_device!=\"nan\"][[\"user\",\"day\",\"hour\",\"op_device\"]].drop_duplicates()\n",
    "dev_per_hour_cnt=pd.DataFrame(dev_per_hour_cnt.groupby([\"user\",\"day\",\"hour\"])[\"op_device\"].nunique()).reset_index()\n",
    "dev_per_hour_cnt=dev_per_hour_cnt[[\"user\",\"op_device\"]]\n",
    "dev_per_hour_cnt=dev_per_hour_cnt.groupby(['user'])[\"op_device\"].agg([\n",
    "    (\"dev_hour_nun_mean\",\"mean\"),\n",
    "    (\"dev_hour_nun_max\",\"max\"),\n",
    "    (\"dev_hour_nun_min\",\"min\"),\n",
    "    (\"dev_hour_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(dev_per_hour_cnt, on='user', how='left')\n",
    "del dev_per_hour_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per day ip nunique\n",
    "ip_per_day_cnt=op_df[op_df.ip!=\"nan\"][[\"user\",\"day\",\"ip\"]].drop_duplicates()\n",
    "ip_per_day_cnt=pd.DataFrame(ip_per_day_cnt.groupby([\"user\",\"day\"])[\"ip\"].nunique()).reset_index()\n",
    "ip_per_day_cnt=ip_per_day_cnt[[\"user\",\"ip\"]]\n",
    "ip_per_day_cnt=ip_per_day_cnt.groupby(['user'])[\"ip\"].agg([\n",
    "    (\"ip_nun_mean\",\"mean\"),\n",
    "    (\"ip_nun_max\",\"max\"),\n",
    "    (\"ip_nun_mean\",\"mean\"),\n",
    "    (\"ip_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(ip_per_day_cnt, on='user', how='left')\n",
    "del ip_per_day_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per day hour ip nunique\n",
    "ip_per_hour_cnt=op_df[op_df.ip!=\"nan\"][[\"user\",\"day\",\"hour\",\"ip\"]].drop_duplicates()\n",
    "ip_per_hour_cnt=pd.DataFrame(ip_per_hour_cnt.groupby([\"user\",\"day\",\"hour\"])[\"ip\"].nunique()).reset_index()\n",
    "ip_per_hour_cnt=ip_per_hour_cnt[[\"user\",\"ip\"]]\n",
    "ip_per_hour_cnt=ip_per_hour_cnt.groupby(['user'])[\"ip\"].agg([\n",
    "    (\"ip_hour_nun_max\",\"max\"),\n",
    "    (\"ip_hour_nun_min\",\"min\"),\n",
    "    (\"ip_hour_nun_mean\",\"mean\"),\n",
    "    (\"ip_hour_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(ip_per_hour_cnt, on='user', how='left')\n",
    "del ip_per_hour_cnt\n",
    "# ip_per_hour_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per day hour min ip nunique\n",
    "ip_per_min_cnt=op_df[op_df.ip!=\"nan\"][[\"user\",\"day\",\"hour\",\"min\",\"ip\"]].drop_duplicates()\n",
    "ip_per_min_cnt=pd.DataFrame(ip_per_min_cnt.groupby([\"user\",\"day\",\"hour\",\"min\"])[\"ip\"].nunique()).reset_index()\n",
    "ip_per_min_cnt=ip_per_min_cnt[[\"user\",\"ip\"]]\n",
    "ip_per_min_cnt=ip_per_min_cnt.groupby(['user'])[\"ip\"].agg([\n",
    "    (\"ip_min_nun_max\",\"max\"),\n",
    "    (\"ip_min_nun_min\",\"min\"),\n",
    "    (\"ip_min_nun_mean\",\"mean\"),\n",
    "    (\"ip_min_nun_std\",\"std\")\n",
    "]).reset_index()\n",
    "data=data.merge(ip_per_min_cnt, on='user', how='left')\n",
    "del ip_per_min_cnt\n",
    "# ip_per_min_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存特征文件\n",
    "data.to_csv(\"F:/data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96202, 830)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
